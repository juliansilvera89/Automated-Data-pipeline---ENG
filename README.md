# ğŸš€ Automated Data Pipeline â€“ Endâ€‘toâ€‘End Project

## ğŸ“Œ Overview

This project implements a **fully automated data pipeline** following real-world **Data Engineering best practices**. It covers the complete lifecycle of data: ingestion, transformation, storage in a Data Lake, loading into a Data Warehouse, SQL analytics, and **scheduled execution by run date**.

The project is designed to be **production-oriented**, reproducible, and easily migratable to tools like **Apache Airflow**.

---

## ğŸ—ï¸ Architecture

```
Raw Data (CSV)
     â†“
Ingestion & Validation (Python)
     â†“
Transformations & Business Rules
     â†“
Data Lake (Parquet, partitioned by date)
     â†“
Data Warehouse (SQLite)
     â†“
SQL Analytics
     â†“
Scheduler (Daily execution simulation)
```

---

## ğŸ§° Tech Stack

* **Python 3**
* **Pandas** â€“ data processing
* **PyArrow / Parquet** â€“ analytical storage
* **SQLite** â€“ lightweight Data Warehouse
* **Logging** â€“ observability & monitoring
* **Google Colab** â€“ execution environment

---

## ğŸ“ Project Structure

```
data_pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Raw and snapshot data (immutable)
â”‚   â”œâ”€â”€ processed/      # Cleaned data in Parquet (Data Lake)
â”‚   â””â”€â”€ warehouse/      # SQLite Data Warehouse
â”œâ”€â”€ logs/               # Pipeline & scheduler logs
â”œâ”€â”€ scripts/            # (logical separation â€“ merged in notebooks)
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline Stages

### 1ï¸âƒ£ Data Ingestion

* Reads raw CSV data
* Validates file existence and schema
* Prevents empty or corrupted loads
* Logs ingestion metrics

**Key principle:** Raw data is never modified.

---

### 2ï¸âƒ£ Data Transformation

Applied business and data-quality rules:

* Date normalization and coercion
* Country and category standardization
* Price cleaning and numeric casting
* Quantity validation (no nulls or negatives)
* Duplicate removal by `order_id`
* Derived metric: `total_amount`

Output is stored in **Parquet format** for analytical workloads.

---

### 3ï¸âƒ£ Data Lake (Processed Layer)

* Columnar storage (Parquet)
* One file per execution date (`run_date`)
* Enables historical backfills and reprocessing

Example:

```
orders_2024-01-01.parquet
orders_2024-01-02.parquet
orders_2024-01-03.parquet
```

---

### 4ï¸âƒ£ Data Warehouse

* Clean data loaded into SQLite
* Fact table: `fact_orders`
* Ready for SQL analytics and BI tools

---

### 5ï¸âƒ£ SQL Analytics

Example queries implemented:

* Total sales
* Sales by country
* Sales by product category
* Average ticket size

---

### 6ï¸âƒ£ Automation & Scheduling

The pipeline is executed through a **single orchestrator function**, simulating a daily scheduler:

* Parameterized by `run_date`
* Supports historical backfills
* Logs each execution independently
* Fully idempotent

This design mirrors **Airflow DAG execution semantics**.

---

## ğŸ“Š Logging & Observability

* Centralized logs per pipeline and scheduler
* Clear START / SUCCESS / FAILURE states
* Timestamped execution history

Example log:

```
PIPELINE START - run_date=2024-01-02
INGEST - rows: 20000
TRANSFORM - parquet generated
DW - load completed
PIPELINE SUCCESS
```

---

## ğŸ¯ Key Engineering Concepts Demonstrated

* ETL / ELT fundamentals
* Data Lake vs Data Warehouse
* Idempotent pipelines
* Execution date vs system date
* Backfills and reprocessing
* Production-ready logging
* Orchestration design (Airflow-ready)

---

## ğŸš€ How This Scales in Production

This pipeline can be easily migrated to:

* **Apache Airflow** (DAG with daily schedule)
* **Cloud Storage** (S3 / GCS instead of local files)
* **BigQuery / Snowflake** instead of SQLite
* **dbt** for transformations

---

## ğŸ‘¤ Author

**JuliÃ¡n Silvera**
Data / Analytics Engineer

---

## âœ… Status

âœ”ï¸ Project completed
âœ”ï¸ Endâ€‘toâ€‘end automated pipeline
âœ”ï¸ Ready for portfolio & interviews
